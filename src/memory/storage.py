"""Supabase-backed memory storage for Meera Hive Mind."""

from __future__ import annotations

import json
from typing import Any, Dict, List, Optional

import requests
import structlog

from src.memory.nodes import UserIdentity  # still used for identity reconstruction
from src.utils.config import settings

logger = structlog.get_logger()


class MemoryStorage:
    """
    Memory storage using Supabase / Postgres instead of Mongo.

    - Table: hive_knowledge
        id          uuid PK (generated by Postgres)
        user_id     text      -- null => hive-mind / global
        title       text
        content     text not null
        metadata    jsonb default '{}'::jsonb
        created_at  timestamptz default now()

    We deliberately return plain dicts for memories so that the
    LangGraph workflow & Vishnu/Shiva can handle them generically.
    """

    def __init__(self) -> None:
        base_url = settings.supabase_url.rstrip("/")  # e.g. https://xilapyewazpzlvqbbtgl.supabase.co
        service_key = settings.supabase_service_role_key

        if not base_url or not service_key:
            raise RuntimeError(
                "Supabase URL / SERVICE_ROLE key not configured. "
                "Set SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY in your environment."
            )

        self.base_url = base_url
        self.service_key = service_key

        self.session = requests.Session()
        self.session.headers.update(
            {
                "apikey": self.service_key,
                "Authorization": f"Bearer {self.service_key}",
                "Content-Type": "application/json",
                "Accept": "application/json",
                # We need the inserted row back to get its id
                "Prefer": "return=representation",
            }
        )

        logger.info("MemoryStorage initialized", base_url=self.base_url)

    # ------------------------------------------------------------------
    # USER IDENTITY
    # ------------------------------------------------------------------

    def get_user_identity(self, user_id: str) -> Optional[UserIdentity]:
        """
        Fetch user identity profile (if you create a table for it).

        Expected table schema (recommended, but optional):

            create table if not exists user_identities (
                user_id    text primary key,
                profile    jsonb not null,
                created_at timestamptz not null default now(),
                updated_at timestamptz not null default now()
            );

        If the table does not exist yet or you are not using identity
        persistence, this will simply return None.
        """
        try:
            url = f"{self.base_url}/rest/v1/user_identities"
            params = {
                "select": "profile",
                "user_id": f"eq.{user_id}",
                "limit": 1,
            }
            resp = self.session.get(url, params=params, timeout=10)
            if resp.status_code == 404:
                # Table not found or RLS misconfig – treat as "no identity"
                logger.warning("user_identities table not available, skipping identity load")
                return None

            resp.raise_for_status()
            rows = resp.json()
            if not rows:
                return None

            profile = rows[0].get("profile") or {}
            # Construct UserIdentity from stored JSON
            return UserIdentity(**profile)

        except Exception as e:
            logger.error("Failed to get user identity", error=str(e), user_id=user_id)
            return None

    # ------------------------------------------------------------------
    # MEMORY HELPERS
    # ------------------------------------------------------------------

    def _row_to_memory(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """Convert a DB row into a generic memory dict."""
        metadata = row.get("metadata") or {}
        return {
            "memory_id": row.get("id"),
            "user_id": row.get("user_id"),
            "title": row.get("title"),
            "content": row.get("content"),
            "metadata": metadata,
            "created_at": row.get("created_at"),
            # derived flag: global memories have no user_id
            "is_hive_mind": row.get("user_id") is None,
        }

    def _memory_to_payload(self, memory: Any) -> Dict[str, Any]:
        """
        Normalize whatever Shiva passes in (Pydantic model, dataclass, dict)
        into a JSON-serializable payload for hive_knowledge.
        """
        if hasattr(memory, "model_dump"):
            data = memory.model_dump()
        elif hasattr(memory, "dict"):
            data = memory.dict()
        elif isinstance(memory, dict):
            data = dict(memory)
        else:
            raise TypeError(f"Unsupported memory object type: {type(memory)}")

        is_hive_mind = bool(data.get("is_hive_mind", False))

        user_id = data.get("user_id")
        if is_hive_mind:
            user_id = None  # global memory

        payload = {
            "user_id": user_id,
            "title": data.get("title"),
            "content": data.get("content") or "",
            "metadata": data.get("metadata") or {},
        }

        return payload

    # ------------------------------------------------------------------
    # PUBLIC API USED BY RETRIEVER + SHIVA
    # ------------------------------------------------------------------

    def search_memories(
        self,
        query_embedding: Any,  # kept for compatibility, currently unused
        user_id: Optional[str],
        is_hive_mind: bool,
        limit: int,
    ) -> List[Dict[str, Any]]:
        """
        Search memories.

        For now we use a simple "recent first" strategy with filters,
        relying on trigram index / full text search you already added
        at the SQL level, or just ordering by created_at.

        NOTE: query_embedding is accepted but not used yet, so you can
        later plug in pgvector without changing the Vishnu / Retriever code.
        """
        try:
            url = f"{self.base_url}/rest/v1/hive_knowledge"
            params: Dict[str, Any] = {
                "select": "id,user_id,title,content,metadata,created_at",
                "order": "created_at.desc",
                "limit": limit,
            }

            if is_hive_mind:
                # Global memories = rows with user_id IS NULL
                params["user_id"] = "is.null"
            else:
                if user_id:
                    params["user_id"] = f"eq.{user_id}"
                else:
                    # No user_id and not hive mind – nothing to return
                    return []

            resp = self.session.get(url, params=params, timeout=10)
            resp.raise_for_status()
            rows = resp.json()

            memories = [self._row_to_memory(r) for r in rows]
            logger.info(
                "Supabase search_memories",
                count=len(memories),
                is_hive_mind=is_hive_mind,
                user_id=user_id,
            )
            return memories[:limit]

        except Exception as e:
            logger.error(
                "Failed Supabase search_memories, falling back to recent",
                error=str(e),
                is_hive_mind=is_hive_mind,
                user_id=user_id,
            )
            return self.get_recent_memories(user_id=user_id, is_hive_mind=is_hive_mind, limit=limit)

    def get_recent_memories(
        self,
        user_id: Optional[str],
        is_hive_mind: bool,
        limit: int,
    ) -> List[Dict[str, Any]]:
        """Return most recent memories with the same filters as above."""
        try:
            url = f"{self.base_url}/rest/v1/hive_knowledge"
            params: Dict[str, Any] = {
                "select": "id,user_id,title,content,metadata,created_at",
                "order": "created_at.desc",
                "limit": limit,
            }

            if is_hive_mind:
                params["user_id"] = "is.null"
            else:
                if user_id:
                    params["user_id"] = f"eq.{user_id}"
                else:
                    return []

            resp = self.session.get(url, params=params, timeout=10)
            resp.raise_for_status()
            rows = resp.json()

            memories = [self._row_to_memory(r) for r in rows]
            logger.info(
                "Supabase get_recent_memories",
                count=len(memories),
                is_hive_mind=is_hive_mind,
                user_id=user_id,
            )
            return memories[:limit]

        except Exception as e:
            logger.error("Failed to get recent memories", error=str(e), user_id=user_id)
            return []

    def save_memory(self, memory: Any) -> Optional[str]:
        """
        Persist a memory node into hive_knowledge.

        Shiva expects this to return a memory_id (string) or None.
        """
        try:
            payload = self._memory_to_payload(memory)

            url = f"{self.base_url}/rest/v1/hive_knowledge"
            resp = self.session.post(url, data=json.dumps(payload), timeout=10)
            resp.raise_for_status()

            rows = resp.json()
            if not rows:
                logger.warning("Supabase save_memory returned no rows")
                return None

            memory_id = rows[0].get("id")
            logger.info("Memory saved to Supabase", memory_id=memory_id)
            return memory_id

        except Exception as e:
            logger.error("Failed to save memory", error=str(e))
            return None

    # ------------------------------------------------------------------

    def close(self) -> None:
        """Close underlying HTTP session."""
        try:
            self.session.close()
        except Exception:
            pass
        logger.info("MemoryStorage closed")
